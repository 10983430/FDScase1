{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data uitlezen. Eerst getest met een dataset van 1000 tweets. Daarna mini_twitter_geotagged_clean. Gebruik de hele dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Verander in de regel hieronder de locatie van de data\n",
    "tweets = pd.read_json(\"mini_twitter_geotagged_clean.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['CleanText'] = tweets['CleanText'].replace('\\n','')\n",
    "tweets['CleanText'] = tweets['CleanText'].replace('\\t','')\n",
    "tweets['CleanText'] = tweets['CleanText'].replace('amp','')\n",
    "tweets['CleanText'] = tweets['CleanText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanText</th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>geo</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COLLUSION TOGETHER httpstco5GMNZq40V3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-08-12 10:04:02</td>\n",
       "      <td>{'hashtags': [{'text': 'NOJUSTICE', 'indices':...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>2016-08-12 10:04:02.194</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 82496193, 'id_str': '82496193', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will year things should have done eight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-08-12 10:04:21</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>2016-08-12 10:04:21.125</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 44032624, 'id_str': '44032624', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clear deliberately throwing this racein 2007 k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-08-12 10:04:30</td>\n",
       "      <td>{'hashtags': [{'text': 'CNN', 'indices': [0, 4...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>2016-08-12 10:04:30.035</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 769208504, 'id_str': '769208504', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wouldnt recognize came from your mouth they co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-08-12 10:04:46</td>\n",
       "      <td>{'hashtags': [{'text': 'NeverTrump', 'indices'...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>{'created_at': 'Fri Aug 12 04:10:25 +0000 2016...</td>\n",
       "      <td>7.639507e+17</td>\n",
       "      <td>7.639507e+17</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>2016-08-12 10:04:46.265</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 41043316, 'id_str': '41043316', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>know suing someone Thats most beautiful thing ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-08-12 10:04:48</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>2016-08-12 10:04:48.229</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 762090248159371264, 'id_str': '76209024...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           CleanText  contributors  \\\n",
       "0              COLLUSION TOGETHER httpstco5GMNZq40V3           NaN   \n",
       "1            will year things should have done eight           NaN   \n",
       "2  clear deliberately throwing this racein 2007 k...           NaN   \n",
       "3  wouldnt recognize came from your mouth they co...           NaN   \n",
       "4  know suing someone Thats most beautiful thing ...           NaN   \n",
       "\n",
       "  coordinates          created_at  \\\n",
       "0        None 2016-08-12 10:04:02   \n",
       "1        None 2016-08-12 10:04:21   \n",
       "2        None 2016-08-12 10:04:30   \n",
       "3        None 2016-08-12 10:04:46   \n",
       "4        None 2016-08-12 10:04:48   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0  {'hashtags': [{'text': 'NOJUSTICE', 'indices':...              None   \n",
       "1  {'hashtags': [], 'urls': [], 'user_mentions': ...              None   \n",
       "2  {'hashtags': [{'text': 'CNN', 'indices': [0, 4...              None   \n",
       "3  {'hashtags': [{'text': 'NeverTrump', 'indices'...              None   \n",
       "4  {'hashtags': [], 'urls': [], 'user_mentions': ...              None   \n",
       "\n",
       "   favorite_count  favorited filter_level   geo  \\\n",
       "0               0      False          low  None   \n",
       "1               0      False          low  None   \n",
       "2               0      False          low  None   \n",
       "3               0      False          low  None   \n",
       "4               0      False          low  None   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                       quoted_status  quoted_status_id  \\\n",
       "0                                               None               NaN   \n",
       "1                                               None               NaN   \n",
       "2                                               None               NaN   \n",
       "3  {'created_at': 'Fri Aug 12 04:10:25 +0000 2016...      7.639507e+17   \n",
       "4                                               None               NaN   \n",
       "\n",
       "  quoted_status_id_str  retweet_count  retweeted  \\\n",
       "0                  NaN              0      False   \n",
       "1                  NaN              0      False   \n",
       "2                  NaN              0      False   \n",
       "3         7.639507e+17              0      False   \n",
       "4                  NaN              0      False   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text            timestamp_ms  \\\n",
       "0  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO... 2016-08-12 10:04:02.194   \n",
       "1  @HillaryClinton he will do in one year all the... 2016-08-12 10:04:21.125   \n",
       "2  #CNN #newday clear #Trump deliberately throwin... 2016-08-12 10:04:30.035   \n",
       "3  @realDonaldTrump, you wouldn't recognize a lie... 2016-08-12 10:04:46.265   \n",
       "4  \"Kid, you know, suing someone? Thats the most ... 2016-08-12 10:04:48.229   \n",
       "\n",
       "  truncated                                               user  \n",
       "0     False  {'id': 82496193, 'id_str': '82496193', 'name':...  \n",
       "1     False  {'id': 44032624, 'id_str': '44032624', 'name':...  \n",
       "2     False  {'id': 769208504, 'id_str': '769208504', 'name...  \n",
       "3     False  {'id': 41043316, 'id_str': '41043316', 'name':...  \n",
       "4     False  {'id': 762090248159371264, 'id_str': '76209024...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we maken een array aan met alle clean tekst uit de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['COLLUSION TOGETHER httpstco5GMNZq40V3',\n",
      "       'will year things should have done eight',\n",
      "       'clear deliberately throwing this racein 2007 knew that destabilization Mideast started wIraq invasion',\n",
      "       ..., 'clear want them write what want sorry Donny',\n",
      "       'picked staring straight into face base your party pretty NeverTrump',\n",
      "       'Vote Jill Stein Trump wins 4yra Trumps hell better than stopping Hillary'],\n",
      "      dtype=object)\n"
     ]
    }
   ],
   "source": [
    "doc_complete = tweets['CleanText'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We moeten nog lemmatizen en stopwoorden verwijderen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Create a set of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create a set of punctuation words \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# This is the function makeing the lemmatization\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# In this function we perform the entire cleaning\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# This is the clean corpus.\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maak een dictionary aan voor alle woorden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA toepassen\n",
    "pas de num_topics aan voor het aantal topics: getest op een paar topics tot 100. Meer geeft een duidelijkere groepering, moeten nog kijken wat we kunnen gebruiken voor het verslag.\n",
    "Hoe hoger het aantal passes hoe beter het resultaat. Passes getest op 50. Dat duurt ong 15 minuten op mijn laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=20, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nu kunnen we de topics printen\n",
    "zorg dat num_topics hetzelfde is als hierboven.\n",
    "num_words laat het aantal woorden zien per topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 -> (0, '0.046*\"obama\" + 0.042*\"show\" + 0.040*\"return\" + 0.037*\"tax\" + 0.036*\"trump\"')\n",
      "Topic 1 -> (1, '0.032*\"woman\" + 0.026*\"point\" + 0.024*\"someone\" + 0.020*\"potus\" + 0.018*\"else\"')\n",
      "Topic 2 -> (2, '0.080*\"medium\" + 0.043*\"never\" + 0.029*\"truth\" + 0.026*\"news\" + 0.025*\"trump\"')\n",
      "Topic 3 -> (3, '0.068*\"youre\" + 0.059*\"donald\" + 0.043*\"trump\" + 0.028*\"candidate\" + 0.022*\"talking\"')\n",
      "Topic 4 -> (4, '0.050*\"know\" + 0.035*\"doesnt\" + 0.034*\"cant\" + 0.032*\"president\" + 0.031*\"word\"')\n",
      "Topic 5 -> (5, '0.031*\"republican\" + 0.030*\"trump\" + 0.028*\"sure\" + 0.028*\"still\" + 0.025*\"hell\"')\n",
      "Topic 6 -> (6, '0.037*\"back\" + 0.032*\"wont\" + 0.030*\"talk\" + 0.028*\"maybe\" + 0.021*\"job\"')\n",
      "Topic 7 -> (7, '0.036*\"press\" + 0.033*\"ever\" + 0.024*\"always\" + 0.021*\"crowd\" + 0.021*\"liberal\"')\n",
      "Topic 8 -> (8, '0.079*\"please\" + 0.059*\"maga\" + 0.051*\"stop\" + 0.043*\"help\" + 0.041*\"nevertrump\"')\n",
      "Topic 9 -> (9, '0.044*\"election\" + 0.036*\"campaign\" + 0.025*\"message\" + 0.023*\"money\" + 0.021*\"done\"')\n",
      "Topic 10 -> (10, '0.024*\"first\" + 0.019*\"comment\" + 0.018*\"yeah\" + 0.017*\"since\" + 0.017*\"guess\"')\n",
      "Topic 11 -> (11, '0.031*\"email\" + 0.031*\"lying\" + 0.023*\"fact\" + 0.023*\"issue\" + 0.020*\"corruption\"')\n",
      "Topic 12 -> (12, '0.076*\"vote\" + 0.064*\"trump\" + 0.039*\"good\" + 0.030*\"need\" + 0.028*\"support\"')\n",
      "Topic 13 -> (13, '0.030*\"speech\" + 0.028*\"losing\" + 0.027*\"like\" + 0.026*\"loser\" + 0.019*\"thanks\"')\n",
      "Topic 14 -> (14, '0.065*\"people\" + 0.056*\"think\" + 0.046*\"dont\" + 0.035*\"want\" + 0.033*\"many\"')\n",
      "Topic 15 -> (15, '0.052*\"poll\" + 0.043*\"like\" + 0.036*\"voter\" + 0.035*\"look\" + 0.028*\"trumppence16\"')\n",
      "Topic 16 -> (16, '0.029*\"voting\" + 0.027*\"believe\" + 0.026*\"everyone\" + 0.022*\"enough\" + 0.022*\"black\"')\n",
      "Topic 17 -> (17, '0.083*\"clinton\" + 0.056*\"hillary\" + 0.045*\"love\" + 0.030*\"corrupt\" + 0.027*\"crooked\"')\n",
      "Topic 18 -> (18, '0.050*\"today\" + 0.044*\"2016\" + 0.036*\"pressure\" + 0.035*\"rain\" + 0.035*\"forecast\"')\n",
      "Topic 19 -> (19, '0.062*\"make\" + 0.059*\"america\" + 0.048*\"time\" + 0.046*\"thats\" + 0.042*\"great\"')\n"
     ]
    }
   ],
   "source": [
    "# Print 2 topics and describe then with 4 words.\n",
    "topics = ldamodel.print_topics(num_topics=20, num_words=5)\n",
    "\n",
    "i=0\n",
    "for topic in topics:\n",
    "    print (\"Topic\",i ,\"->\", topic)     \n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
